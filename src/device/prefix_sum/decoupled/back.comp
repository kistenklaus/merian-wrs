#version 460

#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_memory_scope_semantics : enable
// #extension GL_EXT_shader_atomic_float : enable
#extension GL_KHR_shader_subgroup_vote : enable
#extension GL_KHR_shader_subgroup_ballot : enable
#pragma use_vulkan_memory_model

layout(local_size_x_id = 0, local_size_y = 1, local_size_z = 1) in;

layout(constant_id = 0) const uint WORKGROUP_SIZE = 1;
layout(constant_id = 1) const uint ROWS = 1;
layout(constant_id = 2) const uint SUBGROUP_SIZE = 32;
const uint PARTITION_SIZE = WORKGROUP_SIZE * ROWS;
const uint SUBPARTITION_SIZE = SUBGROUP_SIZE * ROWS;
const uint MAX_NUMBER_OF_SUBGROUPS = (WORKGROUP_SIZE + SUBGROUP_SIZE - 1) / SUBGROUP_SIZE;

layout(set = 0, binding = 0) readonly buffer inElements {
    float elements[];
};

layout(set = 0, binding = 1) writeonly buffer outPrefixSum {
    float prefixSum[];
};

#define state_t uint
#define STATE_NOT_READY 0u
#define STATE_AGGREGATE_PUBLISHED 1u
#define STATE_PREFIX_PUBLISHED 2u
#define STATE_DONT_CARE 3u

struct DecoupledState {
    float aggregate;
    float prefix;
    state_t state;
};

layout(set = 0, binding = 2) volatile buffer DecoupledStates {
    uint counter;
    DecoupledState partitions[];
};

layout(push_constant) uniform PushConstant {
    uint N;
} pc;

#define lbstate_t uint
#define LOOKBACK_STATE_SPIN 1u
#define LOOKBACK_STATE_DONE 0u

shared uint sh_lookBackState;
shared uint sh_partID;

shared float sh_subgroupPrefixSums[MAX_NUMBER_OF_SUBGROUPS];
shared float sh_exclusive;

float localPrefix[ROWS];

float computeLocalPrefixSum(in uint base, in uint N) {
    for (uint i = 0; i < ROWS; ++i) {
        uint index = base + i * SUBGROUP_SIZE;
        if (index < N) {
            localPrefix[i] = elements[index];
        } else {
            localPrefix[i] = 0;
        }
    }

    for (uint i = 0; i < ROWS; ++i) {
        localPrefix[i] = subgroupInclusiveAdd(localPrefix[i]);
    }

    uvec4 ballot = subgroupBallot(base < N);
    uint lastSubgroup = subgroupBallotFindMSB(ballot);

    for (uint i = 1; i < ROWS; ++i) {
        localPrefix[i] += subgroupBroadcast(localPrefix[i - 1], lastSubgroup);
    }

    if (gl_SubgroupInvocationID.x == lastSubgroup) {
        sh_subgroupPrefixSums[gl_SubgroupID.x] = localPrefix[ROWS - 1];
    }
    // Perform tree-based reduction in shared memory
    if (SUBGROUP_SIZE < MAX_NUMBER_OF_SUBGROUPS) {
        for (uint shift = 1; shift <= gl_WorkGroupSize.x; shift <<= 1) {
            barrier();
            if (gl_LocalInvocationID.x >= shift) {
                const float other = sh_subgroupPrefixSums[gl_LocalInvocationID.x - shift];
                sh_subgroupPrefixSums[gl_LocalInvocationID.x] += other;
            }
        }
        barrier();
    } else {
        float groupSum = 0.0;
        if (gl_SubgroupID == 0) {
            groupSum = (gl_SubgroupInvocationID < MAX_NUMBER_OF_SUBGROUPS)
                ? sh_subgroupPrefixSums[gl_SubgroupInvocationID] : 0.0;
            groupSum = subgroupInclusiveAdd(groupSum);
            if (gl_SubgroupInvocationID < MAX_NUMBER_OF_SUBGROUPS) {
                sh_subgroupPrefixSums[gl_SubgroupInvocationID] = groupSum;
            }
        }
        barrier();
    }

    // Step 3: Broadcast the offsets for each subgroup
    float subgroupOffset = (gl_SubgroupID > 0)
        ? sh_subgroupPrefixSums[gl_SubgroupID - 1] : 0.0;

    float exclusive = (gl_SubgroupID > 0) ? sh_subgroupPrefixSums[gl_SubgroupID - 1] : 0.0;
    for (uint i = 0; i < ROWS; ++i) {
        localPrefix[i] += exclusive;
    }

    float workgroupSum = sh_subgroupPrefixSums[gl_NumSubgroups - 1];

    return workgroupSum;
}

// see https://research.nvihttps://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-backdia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back for the
// original paper by NVIDIA.
float decoupledLookback(in uint partID, in float aggregate) {
    // ================ Publish aggregate & state =====================
    if (gl_LocalInvocationID.x == gl_WorkGroupSize.x - 1) {
        // Non atomic write to coherent values.
        partitions[partID].aggregate = aggregate;
        state_t state = STATE_AGGREGATE_PUBLISHED;
        if (partID == 0) {
            partitions[partID].prefix = aggregate;
            state = STATE_PREFIX_PUBLISHED;
        }
        // This atomicStore creates a *happens-before* relationship
        // with following atomicLoads, further the atomic release semantics
        // ensures that all coherent writes before the release operation are visible
        // after a atomic acquire operation of that value.
        // NOTE: gl_SemanticsMakeAvailable is only required if aggregate and prefix
        // are stored in a seperate storage buffer.
        atomicStore(partitions[partID].state, state, gl_ScopeQueueFamily,
            gl_StorageSemanticsBuffer, gl_SemanticsRelease);
    }
    // ================ Decoupled lookback ==================
    float exclusive = 0;
    if (partID != 0) {
        uint lookBackIndex = partID - 1;
        sh_lookBackState = LOOKBACK_STATE_SPIN;
        while (true) {
            state_t predecessorState;
            if (gl_LocalInvocationID.x == gl_WorkGroupSize.x - 1) {
                // Performs a atomic acquire this creates a *happens-before* relationship
                // with the inital atomic acquire above of the acquire below the while loop.
                // Further the atomic acquire ensures that all following coherent reads,
                // are guaranteed to read values written before the corresponding release operation.
                // NOTE: gl_SemanticsMakeVisible is only required if aggregate and prefix
                // are stored in a seperate storage buffer.
                predecessorState = atomicLoad(partitions[lookBackIndex].state,
                        gl_ScopeQueueFamily,
                        gl_StorageSemanticsBuffer, gl_SemanticsAcquire);
                if (predecessorState == STATE_AGGREGATE_PUBLISHED) {
                    // "A -- The predecessor’s aggregate field is added to
                    // exclusive_prefix and the processor continues on to
                    // inspect the preceding tile" - NVIDIA paper.
                    exclusive += partitions[lookBackIndex].aggregate;
                    lookBackIndex -= 1;
                    sh_lookBackState = LOOKBACK_STATE_SPIN;
                } else if (predecessorState == STATE_PREFIX_PUBLISHED) {
                    // "P -- The predecessor’s inclusive_prefix field is added to
                    // exclusive_prefix and the look-back phase is terminated." - NVIDIA paper.
                    exclusive += partitions[lookBackIndex].prefix;
                    sh_lookBackState = LOOKBACK_STATE_DONE;
                } else if (predecessorState == STATE_NOT_READY) {
                    // "X -- Block (or continue polling) until the status_flag is
                    // not X." - NVIDIA paper.
                    sh_lookBackState = LOOKBACK_STATE_SPIN;
                }
            }
            barrier();
            lbstate_t lbstate = sh_lookBackState;
            if (lbstate == LOOKBACK_STATE_DONE) {
                break;
            }
        }
        if (gl_LocalInvocationID.x == gl_WorkGroupSize.x - 1) {
            // Non atomic write to coherent memory.
            partitions[partID].prefix = exclusive + aggregate;
            sh_exclusive = exclusive;
        }

        barrier();
        if (gl_LocalInvocationID.x == gl_WorkGroupSize.x - 1) {
            // Atomic release operations, which creates a *happens-before* relationship
            // with acquire operations of the same value.
            // Gurantees that the coherent write to prefix is visible after
            // following acquire operations.
            // NOTE: gl_SemanticsMakeAvailable is only required if aggregate and prefix
            // are stored in a seperate storage buffer.
            atomicStore(partitions[partID].state, STATE_PREFIX_PUBLISHED,
                gl_ScopeQueueFamily, gl_StorageSemanticsBuffer,
                gl_SemanticsRelease | gl_SemanticsMakeAvailable);
        }

        exclusive = sh_exclusive;
    }
    return exclusive;
}

void main(void) {
    const uint N = pc.N;

    if (gl_LocalInvocationID.x == gl_WorkGroupSize.x - 1) {
        sh_partID = atomicAdd(counter, 1);
    }
    controlBarrier(gl_ScopeWorkgroup, gl_ScopeWorkgroup, gl_StorageSemanticsShared, gl_SemanticsAcquire);
    uint partID = sh_partID;

    const uint base = partID * PARTITION_SIZE + gl_SubgroupID.x * SUBPARTITION_SIZE + gl_SubgroupInvocationID.x;
    //
    float workgroupAggregate = computeLocalPrefixSum(base, N);

    float exclusive = decoupledLookback(partID, workgroupAggregate);

    for (uint i = 0; i < ROWS; ++i) {
        uint index = base + i * SUBGROUP_SIZE;
        if (index < N) {
            prefixSum[index] = exclusive + localPrefix[i];
        }
    }
}
