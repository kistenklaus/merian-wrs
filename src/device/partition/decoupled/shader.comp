#version 460

#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_memory_scope_semantics : enable
// #extension GL_EXT_shader_atomic_float : enable
#extension GL_KHR_shader_subgroup_vote : enable
// #extension GL_KHR_shader_subgroup_ballot : enable
#pragma use_vulkan_memory_model

layout(local_size_x_id = 0, local_size_y = 1, local_size_z = 1) in;
layout(constant_id = 0) const uint WORKGROUP_SIZE = 512;
layout(constant_id = 1) const uint ROWS = 8;
layout(constant_id = 2) const uint SUBGROUP_SIZE = 32;
layout(constant_id = 3) const uint PARALLEL_LOOKBACK_DEPTH = 32;

#include "subgroup_scan.comp"
#include "block_scan.comp"

const uint MAX_SUBGROUPS_PER_WORKGROUP = (WORKGROUP_SIZE + SUBGROUP_SIZE - 1) / SUBGROUP_SIZE;
const uint BLOCK_SIZE = WORKGROUP_SIZE * ROWS;

#ifdef USE_UINT
#define MONOID uint
#elif defined(USE_FLOAT)
#define MONOID float
#else
#define MONOID void
#endif

layout(set = 0, binding = 0) readonly buffer in_Elements {
    MONOID elements[];
};

layout(set = 0, binding = 1) readonly buffer in_Pivot {
    MONOID pivot;
} condition;

// enum State BEGIN
#define state_t uint
const state_t STATE_NOT_READY = 0;
const state_t STATE_AGGREGATE_PUBLISHED = 1;
const state_t STATE_PREFIX_PUBLISHED = 2;
// END
struct DecoupledState {
    uint count;
    uint inclusiveCount;
    state_t state;
};

layout(set = 0, binding = 2) coherent buffer intercom_DecoupledStates {
    uint counter;
    DecoupledState blocks[];
};

layout(set = 0, binding = 3) writeonly buffer out_PartitionIndicies {
    uint partitionIndices[];
};

layout(set = 0, binding = 4) writeonly buffer out_PartitionElements {
    MONOID partitionElements[];
};

layout(set = 0, binding = 5) writeonly buffer out_HeavyCount {
    uint heavyCount;
};

layout(push_constant) uniform PushConstant {
    uint N;
} pc;

shared uint strided_scatch[MAX_SUBGROUPS_PER_WORKGROUP];
MONOID pivot;
uint N;

uint h[ROWS];
MONOID v[ROWS];
uint threadExclusiveCount;

uint blockScan(uint blockID) {
    N = pc.N;
    pivot = condition.pivot;

    const uint blockBase = blockID * BLOCK_SIZE;
    const uint base = blockBase + gl_SubgroupID * (ROWS * SUBGROUP_SIZE) + gl_SubgroupInvocationID.x;

    uint exclusiveCount = 0;
    #pragma unroll
    for (uint i = 0, ix = base; i < ROWS; ++i, ix += SUBGROUP_SIZE) {
        v[i] = ix < N ? elements[ix] : 0;
    }

    #pragma unroll
    for (uint i = 0, ix = base; i < ROWS; ++i, ix += SUBGROUP_SIZE) {
        bool condition = v[i] > pivot;
        uint count;
        h[i] = subgroup_exclusive_scan_bool(condition, count) + exclusiveCount;
        exclusiveCount += count;
    }

    if (gl_SubgroupInvocationID == SUBGROUP_SIZE - 1) {
        strided_scatch[gl_SubgroupID] = exclusiveCount; // inclusive at this point!
    }
    barrier();

    if (gl_SubgroupID == 0) {
        uint subgroupCount = (gl_SubgroupInvocationID < MAX_SUBGROUPS_PER_WORKGROUP)
            ? strided_scatch[gl_SubgroupInvocationID] : 0;
        uint subgroupExclusiveCount = subgroup_inclusive_scan_uint(subgroupCount);
        if (gl_SubgroupInvocationID < MAX_SUBGROUPS_PER_WORKGROUP) {
            strided_scatch[gl_SubgroupInvocationID] = subgroupExclusiveCount;
        }
    }

    barrier();

    return strided_scatch[gl_NumSubgroups - 1];
}

void writePartitionAndIndicies(uint blockID, uint blockExclusiveCount) {
    const uint subgroupExclusiveCount = (gl_SubgroupID > 0) ? strided_scatch[gl_SubgroupID - 1] : 0;

    uint exclusiveCount = subgroupExclusiveCount + blockExclusiveCount;

    for (uint i = 0; i < ROWS; ++i) {
        h[i] += exclusiveCount;
    }

    const uint blockBase = blockID * BLOCK_SIZE;
    const uint base = blockBase + gl_SubgroupID * (ROWS * SUBGROUP_SIZE) + gl_SubgroupInvocationID.x;

    for (uint i = 0, ix = base; i < ROWS && ix < N; ++i, ix += SUBGROUP_SIZE) {
        if (v[i] > pivot) {
            uint hx = h[i];
            partitionIndices[hx] = ix;
            partitionElements[hx] = v[i];
        } else {
            uint l = ix - h[i];
            uint lx = (N - 1) - l;
            partitionIndices[lx] = ix;
            partitionElements[lx] = v[i];
        }
    }
}

shared uint sh_partID;
/// Reduces required forward progess guarantees from LOBE to only OBE.
uint getWorkGroupID() {
    if (gl_LocalInvocationID.x == gl_WorkGroupSize.x - 1) {
        sh_partID = atomicAdd(counter, 1);
    }
    controlBarrier(gl_ScopeWorkgroup, gl_ScopeWorkgroup, gl_StorageSemanticsShared, gl_SemanticsAcquireRelease);
    return sh_partID;
}

// enum LookBackState BEGIN
#define lbstate_t uint
#define LOOKBACK_STATE_SPIN 1u
#define LOOKBACK_STATE_DONE 0u
// END

shared uint sh_lookBackState;
shared uint sh_exclusive;

bool ballotIsZero(in uvec4 ballot) {
    return (ballot.x | ballot.y | ballot.z | ballot.w) == 0;
}

// see https://research.nvihttps://research.nvidia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-backdia.com/publication/2016-03_single-pass-parallel-prefix-scan-decoupled-look-back for the
// original paper by NVIDIA.
uint decoupledLookback(in uint blockID, in uint count) {
    // == Publish aggregate & state
    if (gl_LocalInvocationID.x == 0) {
        // Non atomic write to coherent values.
        blocks[blockID].count = count;
        state_t state = STATE_AGGREGATE_PUBLISHED;
        if (blockID == 0) {
            blocks[blockID].inclusiveCount = count;
            state = STATE_PREFIX_PUBLISHED;
        }
        // This atomicStore creates a *happens-before* relationship
        // with following atomicLoads, further the atomic release semantics
        // ensures that all coherent writes before the release operation are visible
        // after a atomic acquire operation of that value.
        // NOTE: gl_SemanticsMakeAvailable is only required if aggregate and prefix
        // are stored in a seperate storage buffer.
        atomicStore(blocks[blockID].state, state, gl_ScopeQueueFamily,
            gl_StorageSemanticsBuffer, gl_SemanticsRelease);
    }
    // == Decoupled lookback
    uint exclusive = 0;
    uint i = 0;
    const uint MAX_IT = 100000;
    if (blockID != 0) {
        if (gl_SubgroupID == 0) {
            uint lookBackBase = blockID - 1;

            while (true) {
                bool invocActive = gl_SubgroupInvocationID <= lookBackBase && gl_SubgroupInvocationID < PARALLEL_LOOKBACK_DEPTH;

                state_t predecessorState;
                bool done = false;
                if (invocActive) {
                    uint lookBackIdx = lookBackBase - gl_SubgroupInvocationID;
                    predecessorState = atomicLoad(blocks[lookBackIdx].state,
                            gl_ScopeQueueFamily, gl_StorageSemanticsBuffer, gl_SemanticsAcquire);

                    const bool notReady = predecessorState == STATE_NOT_READY;
                    const uvec4 notReadyBallot = subgroupBallot(notReady);
                    uint steps;
                    if (ballotIsZero(notReadyBallot)) {
                        done = predecessorState == STATE_PREFIX_PUBLISHED;
                        const uvec4 doneBallot = subgroupBallot(done);
                        if (ballotIsZero(doneBallot)) {
                            steps = PARALLEL_LOOKBACK_DEPTH;
                        } else {
                            const uint stepsUntilPrefix = subgroupBallotFindLSB(doneBallot) + 1;
                            steps = stepsUntilPrefix;
                        }
                    } else {
                        done = false;
                        const uint stepsUntilNotReady = subgroupBallotFindLSB(notReadyBallot) + 1;
                        steps = stepsUntilNotReady - 1;
                    }
                    if (gl_SubgroupInvocationID.x < steps) {
                        uint acc;
                        if (done) {
                            acc = blocks[lookBackIdx].inclusiveCount;
                        } else {
                            acc = blocks[lookBackIdx].count;
                        }
                        acc = subgroupAdd(acc);
                        if (subgroupElect()) {
                            lookBackBase -= steps;
                            exclusive += acc;
                        }
                    }
                }
                if (subgroupAny(done)) {
                    break;
                }
                lookBackBase = subgroupBroadcastFirst(lookBackBase);

                i++;
                if (i >= MAX_IT) {
                    break;
                }
            }
            if (subgroupElect()) {
                uint inclusive = exclusive + count;
                blocks[blockID].inclusiveCount = inclusive;
                sh_exclusive = exclusive;
            }

            if (subgroupElect()) {
                atomicStore(blocks[blockID].state, STATE_PREFIX_PUBLISHED,
                    gl_ScopeQueueFamily, gl_StorageSemanticsBuffer,
                    gl_SemanticsRelease | gl_SemanticsMakeAvailable);
            }
        }
        // subgroupBarrier();
        controlBarrier(gl_ScopeWorkgroup, gl_ScopeWorkgroup, gl_StorageSemanticsShared, gl_SemanticsAcquireRelease);
        exclusive = sh_exclusive;
    }
    return exclusive;
}

void main(void) {
    const uint gid = gl_GlobalInvocationID.x;
    const uint invocID = gl_LocalInvocationID.x;

    uint blockID = gl_WorkGroupID.x;

    uint blockHeavyCount = blockScan(blockID);

    const uint lastBlockID = (N + BLOCK_SIZE - 1) / BLOCK_SIZE - 1;
    bool lastBlock = blockID == lastBlockID;

    uint blockExclusiveCount = decoupledLookback(blockID, blockHeavyCount);

    writePartitionAndIndicies(blockID, blockExclusiveCount);

    if (lastBlock) {
      heavyCount = blockExclusiveCount + blockHeavyCount;
    }

}
